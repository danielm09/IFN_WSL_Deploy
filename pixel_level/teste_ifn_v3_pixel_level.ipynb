{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb gdown timm h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import geopandas as gpd\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "from convnextv2_unet import ConvNeXtV2_unet\n",
    "from simple_unet import UNet\n",
    "from training_utils import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 15qEZ6nMJ1xLD5l4VQ3bUR21FVDq_QTBU\n",
    "!gdown --id 1a3cGzN6xncf7BH3TmFwqHtG9tOk_gXO7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoraesd90\u001b[0m (\u001b[33mt5_ssl4eo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_path, transform=None):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.h5file = h5py.File(hdf5_path, 'r')\n",
    "        self.size = self.h5file['labels'].shape[0] #.size\n",
    "        self.data = torch.from_numpy(self.h5file['crops'][:].astype(np.float32) / 10000.0)\n",
    "        self.labels = torch.as_tensor(self.h5file['labels'][:],dtype=torch.long)\n",
    "        self.labels[self.labels==255]=20 #added for test with weights - TEMP\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        crop = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            #label = label.unsqueeze(0) # Add a channel dimension to the label\n",
    "            # Concatenate the label as an additional channel to the crop\n",
    "            combined = torch.cat((crop, label.unsqueeze(0)), dim=0)\n",
    "            combined = self.transform(combined)\n",
    "\n",
    "            # Split the crop and label back into separate tensors\n",
    "            crop = combined[:-1]  # All but the last channel\n",
    "            label = combined[-1].long()  # The last channel\n",
    "\n",
    "        return crop, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    #v2.RandomSolarize(0.05)\n",
    "    #v2.ToTensor()  # Convert image to PyTorch tensor\n",
    "])\n",
    "#train_transforms = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchinfo import summary\n",
    "#model = model.cuda() #send model to device\n",
    "#summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HDF5Dataset(\"crops_train_seg_all_sel.hdf5\", transform=train_transforms)\n",
    "test_set = HDF5Dataset(\"crops_test_seg_all_sel.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, train_loader, test_loader, test_eval, optimizer, criterion, config_wandb):\n",
    "\n",
    "    wandb.init(project=\"ifn-weakly-supervised-seg-v2-vast\", config=config_wandb)\n",
    "    \n",
    "    for epoch in range(config_wandb['epochs']):\n",
    "        tstart = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Transfer to GPU \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            mask = labels!=20\n",
    "            labels_selected = labels[mask]\n",
    "            predicted_selected = predicted[mask]\n",
    "            correct += (predicted_selected == labels_selected).sum().item()  # Sum the correct predictions\n",
    "            total += labels_selected.size(0)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        if test_eval:\n",
    "            val_loss, val_accuracy = evaluate(model, test_loader, criterion, num_classes)\n",
    "        else:\n",
    "            val_loss, val_accuracy = (0, 0)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_loss, \"train_acc\":correct/total, \"val_loss\":val_loss, \"val_acc\":val_accuracy})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Acc: {correct / total :.4f}, Test_eval: {str(test_eval)}, Time/epoch: {round((time.time()-tstart)/60,2)}min\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/Desktop/test_torch_SSL/wandb/run-20240820_122444-rh1f751o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1/runs/rh1f751o' target=\"_blank\">zesty-wind-18</a></strong> to <a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1' target=\"_blank\">https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1/runs/rh1f751o' target=\"_blank\">https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1/runs/rh1f751o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Average Loss: 2.6746, Acc: 0.1252, Test_eval: True, Time/epoch: 0.61min\n",
      "Epoch [2/200], Average Loss: 2.5753, Acc: 0.1583, Test_eval: True, Time/epoch: 0.54min\n",
      "Epoch [3/200], Average Loss: 2.5218, Acc: 0.1754, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [4/200], Average Loss: 2.4930, Acc: 0.1837, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [5/200], Average Loss: 2.4649, Acc: 0.1949, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [6/200], Average Loss: 2.4406, Acc: 0.2046, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [7/200], Average Loss: 2.4268, Acc: 0.2115, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [8/200], Average Loss: 2.4034, Acc: 0.2173, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [9/200], Average Loss: 2.3857, Acc: 0.2232, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [10/200], Average Loss: 2.3714, Acc: 0.2281, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [11/200], Average Loss: 2.3615, Acc: 0.2324, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [12/200], Average Loss: 2.3447, Acc: 0.2348, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [13/200], Average Loss: 2.3315, Acc: 0.2407, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [14/200], Average Loss: 2.3129, Acc: 0.2449, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [15/200], Average Loss: 2.3082, Acc: 0.2471, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [16/200], Average Loss: 2.2970, Acc: 0.2493, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [17/200], Average Loss: 2.2916, Acc: 0.2538, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [18/200], Average Loss: 2.2819, Acc: 0.2559, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [19/200], Average Loss: 2.2678, Acc: 0.2583, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [20/200], Average Loss: 2.2639, Acc: 0.2625, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [21/200], Average Loss: 2.2570, Acc: 0.2656, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [22/200], Average Loss: 2.2405, Acc: 0.2680, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [23/200], Average Loss: 2.2441, Acc: 0.2688, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [24/200], Average Loss: 2.2323, Acc: 0.2715, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [25/200], Average Loss: 2.2238, Acc: 0.2739, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [26/200], Average Loss: 2.2130, Acc: 0.2766, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [27/200], Average Loss: 2.2063, Acc: 0.2789, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [28/200], Average Loss: 2.1998, Acc: 0.2807, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [29/200], Average Loss: 2.1942, Acc: 0.2824, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [30/200], Average Loss: 2.1898, Acc: 0.2843, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [31/200], Average Loss: 2.1809, Acc: 0.2860, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [32/200], Average Loss: 2.1780, Acc: 0.2868, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [33/200], Average Loss: 2.1684, Acc: 0.2904, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [34/200], Average Loss: 2.1610, Acc: 0.2925, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [35/200], Average Loss: 2.1660, Acc: 0.2939, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [36/200], Average Loss: 2.1537, Acc: 0.2953, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [37/200], Average Loss: 2.1490, Acc: 0.2963, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [38/200], Average Loss: 2.1423, Acc: 0.2979, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [39/200], Average Loss: 2.1450, Acc: 0.2968, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [40/200], Average Loss: 2.1446, Acc: 0.2996, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [41/200], Average Loss: 2.1232, Acc: 0.3032, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [42/200], Average Loss: 2.1218, Acc: 0.3052, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [43/200], Average Loss: 2.1164, Acc: 0.3058, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [44/200], Average Loss: 2.1131, Acc: 0.3073, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [45/200], Average Loss: 2.1116, Acc: 0.3092, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [46/200], Average Loss: 2.1025, Acc: 0.3106, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [47/200], Average Loss: 2.1003, Acc: 0.3109, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [48/200], Average Loss: 2.1001, Acc: 0.3100, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [49/200], Average Loss: 2.0899, Acc: 0.3134, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [50/200], Average Loss: 2.0875, Acc: 0.3157, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [51/200], Average Loss: 2.0867, Acc: 0.3164, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [52/200], Average Loss: 2.0776, Acc: 0.3167, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [53/200], Average Loss: 2.0667, Acc: 0.3220, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [54/200], Average Loss: 2.0730, Acc: 0.3184, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [55/200], Average Loss: 2.0624, Acc: 0.3228, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [56/200], Average Loss: 2.0632, Acc: 0.3220, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [57/200], Average Loss: 2.0508, Acc: 0.3244, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [58/200], Average Loss: 2.0543, Acc: 0.3251, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [59/200], Average Loss: 2.0447, Acc: 0.3275, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [60/200], Average Loss: 2.0426, Acc: 0.3285, Test_eval: True, Time/epoch: 0.53min\n",
      "Epoch [61/200], Average Loss: 2.0437, Acc: 0.3273, Test_eval: True, Time/epoch: 0.56min\n",
      "Epoch [62/200], Average Loss: 2.0369, Acc: 0.3283, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [63/200], Average Loss: 2.0274, Acc: 0.3312, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [64/200], Average Loss: 2.0206, Acc: 0.3336, Test_eval: True, Time/epoch: 0.58min\n",
      "Epoch [65/200], Average Loss: 2.0176, Acc: 0.3342, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [66/200], Average Loss: 2.0174, Acc: 0.3350, Test_eval: True, Time/epoch: 0.56min\n",
      "Epoch [67/200], Average Loss: 2.0148, Acc: 0.3378, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [68/200], Average Loss: 2.0055, Acc: 0.3403, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [69/200], Average Loss: 2.0060, Acc: 0.3389, Test_eval: True, Time/epoch: 0.58min\n",
      "Epoch [70/200], Average Loss: 2.0005, Acc: 0.3402, Test_eval: True, Time/epoch: 0.6min\n",
      "Epoch [71/200], Average Loss: 1.9958, Acc: 0.3420, Test_eval: True, Time/epoch: 0.58min\n",
      "Epoch [72/200], Average Loss: 2.0027, Acc: 0.3420, Test_eval: True, Time/epoch: 0.58min\n",
      "Epoch [73/200], Average Loss: 1.9837, Acc: 0.3440, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [74/200], Average Loss: 1.9888, Acc: 0.3440, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [75/200], Average Loss: 1.9828, Acc: 0.3462, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [76/200], Average Loss: 1.9797, Acc: 0.3460, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [77/200], Average Loss: 1.9754, Acc: 0.3487, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [78/200], Average Loss: 1.9731, Acc: 0.3502, Test_eval: True, Time/epoch: 0.54min\n",
      "Epoch [79/200], Average Loss: 1.9657, Acc: 0.3519, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [80/200], Average Loss: 1.9669, Acc: 0.3505, Test_eval: True, Time/epoch: 0.56min\n",
      "Epoch [81/200], Average Loss: 1.9679, Acc: 0.3508, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [82/200], Average Loss: 1.9496, Acc: 0.3542, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [83/200], Average Loss: 1.9550, Acc: 0.3568, Test_eval: True, Time/epoch: 0.56min\n",
      "Epoch [84/200], Average Loss: 1.9542, Acc: 0.3558, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [85/200], Average Loss: 1.9371, Acc: 0.3560, Test_eval: True, Time/epoch: 0.56min\n",
      "Epoch [86/200], Average Loss: 1.9436, Acc: 0.3583, Test_eval: True, Time/epoch: 0.54min\n",
      "Epoch [87/200], Average Loss: 1.9333, Acc: 0.3618, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [88/200], Average Loss: 1.9346, Acc: 0.3621, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [89/200], Average Loss: 1.9326, Acc: 0.3607, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [90/200], Average Loss: 1.9286, Acc: 0.3635, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [91/200], Average Loss: 1.9273, Acc: 0.3626, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [92/200], Average Loss: 1.9216, Acc: 0.3639, Test_eval: True, Time/epoch: 0.54min\n",
      "Epoch [93/200], Average Loss: 1.9152, Acc: 0.3660, Test_eval: True, Time/epoch: 0.55min\n",
      "Epoch [94/200], Average Loss: 1.9022, Acc: 0.3694, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [95/200], Average Loss: 1.9181, Acc: 0.3656, Test_eval: True, Time/epoch: 0.57min\n",
      "Epoch [96/200], Average Loss: 1.8996, Acc: 0.3702, Test_eval: True, Time/epoch: 0.59min\n",
      "Epoch [97/200], Average Loss: 1.8985, Acc: 0.3704, Test_eval: True, Time/epoch: 0.59min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;66;03m#send model to device\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m trainModel(model, train_loader, test_loader, test_eval, optimizer, criterion, config_wandb)\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mtrainModel\u001b[0;34m(model, train_loader, test_loader, test_eval, optimizer, criterion, config_wandb)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 24\u001b[0m outputs_selected \u001b[38;5;241m=\u001b[39m outputs[mask_expanded]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes)\n\u001b[1;32m     25\u001b[0m labels_selected \u001b[38;5;241m=\u001b[39m labels[mask]\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs_selected, labels_selected) \u001b[38;5;66;03m#no need to have softmax applied earlier\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "num_classes = 21\n",
    "\n",
    "ws = [1 for i in range(21)] #loss weights\n",
    "ws[-1] = 0\n",
    "ws = torch.tensor(ws).float().cuda()\n",
    "\n",
    "num_epochs = 150\n",
    "criterion = nn.CrossEntropyLoss(weight=ws)\n",
    "test_eval = True\n",
    "\n",
    "batch_size_list = [32, 64]\n",
    "lr_list = [0.01, 0.001, 0.0001]\n",
    "depths_list = [\n",
    "    [2, 2, 6, 2],\n",
    "]\n",
    "dims_list = [\n",
    "    [40, 80, 160, 320],\n",
    "]\n",
    "\n",
    "for batch_size, lr, depth, dim in product(batch_size_list, lr_list, depths_list, dims_list):\n",
    "    #set data loaders, which will vary according to the batch_size\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=14,pin_memory=True,persistent_workers=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=14,pin_memory=True)\n",
    "    #set model, which will vary according to the depth and dim\n",
    "    model = None #first make sure no previous model was initialized\n",
    "    model = UNet(in_channels=36, out_channels=num_classes)\n",
    "    #model = ConvNeXtV2_unet(in_chans=36, num_classes=20, depths=depth, dims=dim, use_orig_stem=False)\n",
    "    #model = ConvNeXtV2(in_chans=36, num_classes=20, depths=depth, dims=dim)\n",
    "    #set optimizer, which will vary according to the learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    #set wandb config dictionary, which will vary depending on the parameters\n",
    "    config_wandb = {\n",
    "    \"optimizer\": \"Adam\", #fixed\n",
    "    \"criterion\": \"CrossEntropyLoss_ws\", #fixed\n",
    "    \"learning_rate\": lr, \n",
    "    \"epochs\": num_epochs, #fixed\n",
    "    \"batch_size\": batch_size,\n",
    "    \"augmentations\":\"H&V Flip\", #fixed\n",
    "    \"architecture\":\"UNet_Simple\",#\"ConvNextV2_mod\", #fixed\n",
    "    \"depth\": depth,\n",
    "    \"dim\": dim\n",
    "    }\n",
    "\n",
    "    model = model.cuda() #send model to device\n",
    "\n",
    "    #train model\n",
    "    trainModel(model, train_loader, test_loader, test_eval, optimizer, criterion, config_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3012571e3e64194a49bc1ad2e14eb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▂▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇███▇</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▄▃▃▃▂▂▂▁▂▂▂▁▁▁▁▂▁▁▂▂▁▁▂▁▁▁▃▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>96</td></tr><tr><td>train_acc</td><td>0.37039</td></tr><tr><td>train_loss</td><td>1.89854</td></tr><tr><td>val_acc</td><td>0.34236</td></tr><tr><td>val_loss</td><td>2.17764</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-wind-18</strong> at: <a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1/runs/rh1f751o' target=\"_blank\">https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1/runs/rh1f751o</a><br/> View project at: <a href='https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1' target=\"_blank\">https://wandb.ai/t5_ssl4eo/ifn-weakly-supervised-seg-v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240820_122444-rh1f751o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
