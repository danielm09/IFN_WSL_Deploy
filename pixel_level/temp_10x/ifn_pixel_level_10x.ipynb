{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb timm h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import geopandas as gpd\n",
    "import wandb\n",
    "#import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_np = np.load('sen2_65k_181b_means.npy')\n",
    "stds_np = np.load('sen2_65k_181b_stds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_idxs = [i for i in range(180)] + [285]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT LOADING IN MEMORY\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_path, transform=None, standardization=True):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.standardization = standardization\n",
    "        self.h5file = h5py.File(hdf5_path, 'r')\n",
    "        self.size = self.h5file['labels'].shape[0] #.size\n",
    "        #self.data = torch.from_numpy(self.h5file['crops'][:].astype(np.float32) / 10000.0)\n",
    "        #self.labels = torch.as_tensor(self.h5file['labels'][:],dtype=torch.long)\n",
    "        #self.labels[self.labels==255]=20 #added for test with weights - TEMP\n",
    "        self.means = torch.from_numpy(means_np.astype(np.float32)).view(181, 1, 1) \n",
    "        self.stds = torch.from_numpy(stds_np.astype(np.float32)).view(181, 1, 1) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.standardization:\n",
    "            crop = torch.from_numpy(self.h5file['crops'][idx, bands_idxs, :, :].astype(np.float32))\n",
    "            crop = torch.where(crop > 10000, 10000, crop) #normalization\n",
    "            crop = (crop - self.means)/self.stds\n",
    "        else:\n",
    "            crop = torch.from_numpy(self.h5file['crops'][idx, bands_idxs, :, :].astype(np.float32) / 10000.0)#self.data[idx]\n",
    "            crop = torch.where(crop > 1, 1.0, crop) #normalization\n",
    "        \n",
    "        label = torch.as_tensor(self.h5file['labels'][idx],dtype=torch.long)#self.labels[idx]\n",
    "        label[label==255]=20\n",
    "\n",
    "        if self.transform:\n",
    "            #label = label.unsqueeze(0) # Add a channel dimension to the label\n",
    "            # Concatenate the label as an additional channel to the crop\n",
    "            combined = torch.cat((crop, label.unsqueeze(0)), dim=0)\n",
    "            combined = self.transform(combined)\n",
    "\n",
    "            # Split the crop and label back into separate tensors\n",
    "            crop = combined[:-1]  # All but the last channel\n",
    "            label = combined[-1].long()  # The last channel\n",
    "\n",
    "        return crop, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    #v2.RandomCrop(size=(56,56))\n",
    "    #v2.RandomSolarize(0.05)\n",
    "    #v2.ToTensor()  # Convert image to PyTorch tensor\n",
    "])\n",
    "#train_transforms = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HDF5Dataset(\"crops_train_seg_all_augmented.hdf5\", transform=train_transforms, standardization=True)\n",
    "test_set = HDF5Dataset(\"crops_test_seg_all.hdf5\", standardization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, train_loader, test_loader, test_eval, optimizer, criterion, log_to_wandb=True, config_wandb=None):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project=\"ifn-weakly-supervised-seg-v5\", config=config_wandb)\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(config_wandb['epochs']):\n",
    "        tstart = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "        #for inputs, _, labels in train_loader: #uncomment to use dataset with new labels\n",
    "            # Transfer to GPU \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # # Define the mask for loss computation\n",
    "            # mask = labels!=255\n",
    "            # mask_expanded = mask.unsqueeze(1).expand(-1, num_classes, -1, -1)\n",
    "\n",
    "            # # Forward pass\n",
    "            # outputs = model(inputs)\n",
    "            # outputs_selected = outputs[mask_expanded].view(-1, num_classes)\n",
    "            # labels_selected = labels[mask]\n",
    "            # loss = criterion(outputs_selected, labels_selected) #no need to have softmax applied earlier\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # #_, predicted = torch.max(outputs, 1)\n",
    "            # predicted_selected = torch.argmax(outputs_selected, dim=1) #no need to have softmax applied earlier\n",
    "            # total += labels_selected.size(0)\n",
    "            # correct += (predicted_selected == labels_selected).sum().item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            mask = labels!=20\n",
    "            labels_selected = labels[mask]\n",
    "            predicted_selected = predicted[mask]\n",
    "            correct += (predicted_selected == labels_selected).sum().item()  # Sum the correct predictions\n",
    "            total += labels_selected.size(0)\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        if test_eval:\n",
    "            val_loss, val_accuracy = evaluate(model, test_loader, criterion)\n",
    "        else:\n",
    "            val_loss, val_accuracy = (0, 0)\n",
    "        if test_eval and val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            best_model = copy.deepcopy(model)\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": avg_loss, \"train_acc\":correct/total, \"val_loss\":val_loss, \"val_acc\":val_accuracy})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Acc: {correct / total :.4f}, Test_eval: {str(test_eval)}, Test Acc: {val_accuracy:.4f}, Time/epoch: {round((time.time()-tstart)/60,2)}min\")\n",
    "\n",
    "    model_name = f\"Model_{config_wandb['architecture']}_depth{config_wandb['depth']}_dim{config_wandb['dim']}_batch{config_wandb['batch_size']}_lr{str(config_wandb['learning_rate'])[2:]}_Aug{config_wandb['augmentations']}_{config_wandb['optimizer']}_{config_wandb['criterion']}.pt\"\n",
    "    torch.save(best_model, model_name)\n",
    "    if log_to_wandb:\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # Define the mask for loss computation\n",
    "            # mask = labels!=255\n",
    "            # mask_expanded = mask.unsqueeze(1).expand(-1, num_classes, -1, -1)\n",
    "            # outputs = model(inputs)\n",
    "            # outputs_selected = outputs[mask_expanded].view(-1, num_classes)\n",
    "            # labels_selected = labels[mask]\n",
    "            # loss = criterion(outputs_selected, labels_selected)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # predicted_selected = torch.argmax(outputs_selected, dim=1) #no need to have softmax applied earlier\n",
    "            # total += labels_selected.size(0)\n",
    "            # correct += (predicted_selected == labels_selected).sum().item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            mask = labels!=20\n",
    "            labels_selected = labels[mask]\n",
    "            predicted_selected = predicted[mask]\n",
    "            correct += (predicted_selected == labels_selected).sum().item()  # Sum the correct predictions\n",
    "            total += labels_selected.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = torch.load('Model_ConvNextV2_depth[2, 2, 6, 2]_dim[40, 80, 160, 320]_batch64_lr0001_AugH&V_Flip_Adam_MSE_STAND.pt')\n",
    "ssl_model = pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed_names = {k.replace('encoder.',''):v for k, v in ssl_model.items()}\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "def remap_checkpoint_keys(ckpt):\n",
    "    new_ckpt = OrderedDict()\n",
    "    for k, v in ckpt.items():\n",
    "        if k.startswith(\"encoder\"):\n",
    "            k = \".\".join(k.split(\".\")[1:])  # remove encoder in the name\n",
    "        if k.endswith(\"kernel\"):\n",
    "            k = \".\".join(k.split(\".\")[:-1])  # remove kernel in the name\n",
    "            new_k = k + \".weight\"\n",
    "            if len(v.shape) == 3:  # resahpe standard convolution\n",
    "                kv, in_dim, out_dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = (\n",
    "                    v.permute(2, 1, 0).reshape(out_dim, in_dim, ks, ks).transpose(3, 2)\n",
    "                )\n",
    "            elif len(v.shape) == 2:  # reshape depthwise convolution\n",
    "                kv, dim = v.shape\n",
    "                ks = int(math.sqrt(kv))\n",
    "                new_ckpt[new_k] = (\n",
    "                    v.permute(1, 0).reshape(dim, 1, ks, ks).transpose(3, 2)\n",
    "                )\n",
    "            continue\n",
    "        elif \"ln\" in k or \"linear\" in k:\n",
    "            k = k.split(\".\")\n",
    "            k.pop(-2)  # remove ln and linear in the name\n",
    "            new_k = \".\".join(k)\n",
    "        elif \"backbone.resnet\" in k:\n",
    "            # sometimes the resnet model is saved with the prefix backbone.resnet\n",
    "            # we need to remove this prefix\n",
    "            new_k = k.split(\"backbone.resnet.\")[1]\n",
    "        else:\n",
    "            new_k = k\n",
    "        new_ckpt[new_k] = v\n",
    "\n",
    "    # reshape grn affine parameters and biases\n",
    "    for k, v in new_ckpt.items():\n",
    "        if k.endswith(\"bias\") and len(v.shape) != 1:\n",
    "            new_ckpt[k] = v.reshape(-1)\n",
    "        elif \"grn\" in k:\n",
    "            new_ckpt[k] = v.unsqueeze(0).unsqueeze(1)\n",
    "    return new_ckpt\n",
    "\n",
    "fixed_names = remap_checkpoint_keys(ssl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from convnextv2_unet import ConvNeXtV2_unet\n",
    "\n",
    "ws = [1 for i in range(21)] ##TEMP BLOCK - added to experiment loss with weights\n",
    "ws[-1] = 0\n",
    "ws = torch.tensor(ws).float().cuda()\n",
    "\n",
    "# sum_freq = train_set.labels.unique(return_counts=True)[1][:-1].sum() ##TEMP BLOCK - added to experiment with balancing loss weights\n",
    "# ws = [sum_freq/(i*20) for i in train_set.labels.unique(return_counts=True)[1][:-1]]\n",
    "# ws.append(torch.tensor(0.))\n",
    "# ws = torch.tensor(ws).float().cuda()\n",
    "\n",
    "num_classes = 21\n",
    "\n",
    "num_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss(weight=ws)#FocalLoss(gamma=2.5, alpha=ws)#\n",
    "test_eval = True\n",
    "log_to_wandb = False\n",
    "\n",
    "batch_size_list = [32]\n",
    "lr_list = [0.0001]\n",
    "depths_list = [\n",
    "    [2, 2, 6, 2],#[3, 3, 9, 3],\n",
    "]\n",
    "dims_list = [\n",
    "    [40, 80, 160, 320],#[96, 192, 384, 768],\n",
    "]\n",
    "\n",
    "for batch_size, lr, depth, dim in product(batch_size_list, lr_list, depths_list, dims_list):\n",
    "    #set data loaders, which will vary according to the batch_size\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8,pin_memory=True,persistent_workers=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8,pin_memory=True)\n",
    "    #set model, which will vary according to the depth and dim\n",
    "    model = None #first make sure no previous model was initialized\n",
    "    #model = UNet(in_channels=181, out_channels=num_classes, init_features=64)\n",
    "    #model.load_state_dict(pretrained , strict=False)\n",
    "    model = ConvNeXtV2_unet(img_size=64, patch_size=8, in_chans=181, num_classes=num_classes, depths=depth, dims=dim)#, use_orig_stem=False)\n",
    "    model.load_state_dict(fixed_names, strict=False)\n",
    "    #set optimizer, which will vary according to the learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    #set wandb config dictionary, which will vary depending on the parameters\n",
    "    config_wandb = {\n",
    "    \"optimizer\": \"Adam\", #fixed\n",
    "    \"criterion\": \"CE_ws_64_181_aug_STAND_pretrainedSSL_2\", #fixed\n",
    "    \"learning_rate\": lr, \n",
    "    \"epochs\": num_epochs, #fixed\n",
    "    \"batch_size\": batch_size,\n",
    "    \"augmentations\":\"H&V_Flip\", #fixed\n",
    "    \"architecture\":\"ConvNextV2_mod\",#\"ConvNextV2_mod\", \"SimpleUNet\" #fixed\n",
    "    \"depth\": depth,\n",
    "    \"dim\": dim\n",
    "    }\n",
    "\n",
    "    model = model.cuda() #send model to device\n",
    "\n",
    "    #train model\n",
    "    trainModel(model, train_loader, test_loader, test_eval, optimizer, criterion, log_to_wandb, config_wandb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
